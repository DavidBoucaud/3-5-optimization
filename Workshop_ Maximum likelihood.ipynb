{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sympy as sy\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Bisection\n",
    "\n",
    "\n",
    "One of the most common algorithms for numerical root-finding is *bisection*.\n",
    "\n",
    "To understand the idea, recall the well-known game where:\n",
    "\n",
    "- Player A thinks of a secret number between 1 and 100  \n",
    "- Player B asks if it’s less than 50  \n",
    "  \n",
    "  - If yes, B asks if it’s less than 25  \n",
    "  - If no, B asks if it’s less than 75  \n",
    "  \n",
    "\n",
    "And so on.\n",
    "\n",
    "This is bisection, a relative of [binary search](https://en.wikipedia.org/wiki/Binary_search_algorithm). It works for all sufficiently well behaved increasing continuous functions with $ f(a) < 0 < f(b) $. \n",
    "\n",
    "Write an implementation of the bisection algorith, `bisect(f, lower, upper, tol)` which, given a function `f`, a lower bound `lower` and an upper bound `upper` finds the point `x` where `f(x) = 0`. The parameter `tol` is a numerical tolerance, you should stop once your step size is smaller than `tol`.\n",
    "\n",
    "\n",
    "Use it to minimize the function:\n",
    "\n",
    "$$\n",
    "f(x) = \\sin(4 (x - 1/4)) + x + x^{20} - 1 \\tag{2}\n",
    "$$\n",
    "\n",
    "in python: `lambda x: np.sin(4 * (x - 1/4)) + x + x**20 - 1`\n",
    "\n",
    "The value where f(x) = 0 should be around `0.408`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bisect(f, lower, upper, tol):\n",
    "\n",
    "    temp_upper = upper\n",
    "    temp_lower = lower\n",
    "\n",
    "    for step in range(tol):\n",
    "\n",
    "        x = ((temp_upper - temp_lower) / 2 ) + temp_lower\n",
    "        result = f(x)\n",
    "\n",
    "        if result == 0:\n",
    "            return x\n",
    "\n",
    "        if result > 0:\n",
    "            temp_upper = temp_upper - ((temp_upper - temp_lower) / 2 )\n",
    "\n",
    "        else:\n",
    "            temp_lower = temp_lower + ((temp_upper - temp_lower) / 2 ) \n",
    "            \n",
    "    return x\n",
    "\n",
    "f= lambda x: np.sin(4 * (x - 1/4)) + x + x**20 - 1\n",
    "\n",
    "print(bisect(f, 0, 100, 20))\n",
    "# It locks in on 0.4 around 10 steps and improves from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 (stretch) Recursive Bisect\n",
    "\n",
    "Write a recursive version of the bisection algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Movies Regression\n",
    "\n",
    "Write the best linear regression model you can on the [Movies Dataset](https://www.kaggle.com/rounakbanik/the-movies-dataset?select=ratings.csv) to predict the profitability of a movie (revenue - budget). Maintain the interpretability of the model.\n",
    "\n",
    "Few notes:\n",
    "\n",
    "1. Clean your data! Movies where the budget or revenue are invalid should be thrown out\n",
    "\n",
    "2. Be creative with feature engineering. You can include processing to one-hot encode the type of movie, etc.\n",
    "\n",
    "3. The model should be useful for someone **who is thinking about making a movie**. So features like the popularity can't be used. You could, however, use the ratings to figure out if making \"good\" or \"oscar bait\" movies is a profitable strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "meta_df = pd.read_csv(r'C:\\Users\\David\\Documents\\code\\Module 3\\3-5-optimization\\archive\\movies_metadata.csv')\n",
    "\n",
    "#Clean a few key columns\n",
    "meta_df.revenue = pd.to_numeric(meta_df.revenue, errors='coerce')\n",
    "meta_df.budget = pd.to_numeric(meta_df.budget, errors='coerce')\n",
    "meta_df.runtime = pd.to_numeric(meta_df.runtime, errors='coerce')\n",
    "\n",
    "#Extract the genre strings\n",
    "meta_df['genre'] = meta_df.genres.str.findall(pat='([A-Z]\\w*)')\n",
    "\n",
    "#More cleaning\n",
    "meta_df = meta_df.dropna(subset=['budget', 'revenue', 'runtime', 'genres'])\n",
    "meta_df = meta_df[(meta_df.budget != 0)]\n",
    "\n",
    "#Create some new variables\n",
    "meta_df.release_date = pd.to_datetime(meta_df.release_date)\n",
    "meta_df['month'] = meta_df.release_date.dt.strftime(\"%B\")\n",
    "meta_df['profit'] = meta_df['revenue'] - meta_df['budget']\n",
    "\n",
    "#Make a new, cleaner dataframe for regression\n",
    "df = pd.DataFrame(meta_df[['profit', 'runtime', 'budget']])\n",
    "df = df.join(pd.get_dummies(pd.get_dummies(meta_df.genre.apply(pd.Series).stack(), drop_first=True).sum(level=0)))\n",
    "df = df.join(pd.get_dummies(meta_df.month, drop_first=True))\n",
    "\n",
    "#Final clean\n",
    "df = df.replace(-np.inf, np.nan)\n",
    "df = df.dropna()\n",
    "\n",
    "#Run the model\n",
    "y = df.profit\n",
    "x = df.drop(['profit'], axis=1)\n",
    "\n",
    "X = sm.add_constant(x)\n",
    "my_model = sm.OLS(y,X).fit()\n",
    "\n",
    "my_model.summary()"
   ]
  },
  {
   "source": [
    "## Explained:\n",
    "\n",
    "I realize this is far from a good, reliable regression. I kept it because in the chaos, there are some serious takeaways. Within genres and months, we see some with strong coefficients and p-values, which denotes a viable relationship to profit. Others have unreliable p-values and small coefficients, which is worth noting for its non-relationship (it does not increase or reduce profit)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Movies Manual Regression\n",
    "\n",
    "Use your `X` and `y` matrix from 2.1 to calculate the linear regression yourself using the normal equation $(X^T X)^{-1}X^Ty$.\n",
    "\n",
    "Verify that the coefficients are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First attempt: [-1.25913468e+07  3.49901396e+04  1.94049783e+00]\nFirst attempt: const         -2.019091e+07\nruntime        1.657177e+05\nbudget         1.826289e+00\nAdventure      1.080808e+07\nAnimation      2.043897e+07\nComedy        -4.087082e+06\nCrime         -2.670676e+06\nDocumentary    5.356431e+06\nDrama         -6.806587e+06\nFamily         5.264381e+06\nFantasy        1.774105e+06\nFiction       -1.835344e+06\nForeign       -1.706955e+06\nHistory       -2.298770e+07\nHorror         8.683341e+06\nMovie         -4.547748e+06\nMusic          2.499181e+06\nMystery       -2.280183e+06\nRomance        4.859021e+06\nScience       -1.835344e+06\nTV            -4.547748e+06\nThriller      -8.570603e+06\nWar           -9.346423e+06\nWestern       -2.675549e+07\nAugust        -6.376887e+06\nDecember       5.074125e+06\nFebruary      -5.517921e+06\nJanuary       -5.146609e+06\nJuly           2.163225e+06\nJune           2.021745e+07\nMarch         -5.547655e+06\nMay            1.147140e+07\nNovember       5.715161e+06\nOctober       -6.556564e+06\nSeptember     -5.835673e+06\ndtype: float64\n------------------------------------------\nSecond attempt: [-1.25913468e+07  3.49901396e+04  1.94049783e+00]\nSecond attempt: const     -1.259135e+07\nruntime    3.499014e+04\nbudget     1.940498e+00\ndtype: float64\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "normal_matrix = inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "coeffs = my_model.params #This is a catch-all sum of the coefficient, what happens when you move between points\n",
    "\n",
    "print('First attempt:', normal_matrix)\n",
    "print('First attempt:',coeffs)\n",
    "\n",
    "#This is how I started, and clearly the coefficients do not match. Instead, let's simplify the regression.\n",
    "\n",
    "y_ = df.profit\n",
    "x_ = df[['runtime', 'budget']]\n",
    "\n",
    "X_ = sm.add_constant(x_)\n",
    "my_model_ = sm.OLS(y_,X_).fit()\n",
    "\n",
    "normal_matrix_ = inv(X_.T.dot(X_)).dot(X_.T).dot(y_)\n",
    "\n",
    "coeffs_ = my_model_.params #This is a catch-all sum of the coefficient, what happens when you move between points\n",
    "\n",
    "print('------------------------------------------')\n",
    "\n",
    "print('Second attempt:',normal_matrix_)\n",
    "print('Second attempt:',coeffs_)\n",
    "\n",
    "#Now it's working. Clearly my poor regression is not replicable manually. I will continue the workshop with this (simple) regression instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Movies gradient descent regression\n",
    "\n",
    "Use your `X` and `y` matrix from 2.1 to calculate the linear regression yourself using **gradient descent**. \n",
    "\n",
    "Hint: use `scipy.optimize` and remember we're finding the $\\beta$ that minimizes the squared loss function of linear regression: $f(\\beta) = (\\beta X - y)^2$. This will look like part 3 of this lecture.\n",
    "\n",
    "Verify your coefficients are similar to the ones in 2.1 and 2.2. They won't necessarily be exactly the same, but should be roughly similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Gradient descent attempt: [-1.49254181e+08  2.16794411e+07  2.08288458e+08]\nTrue coefficients: const     -1.259135e+07\nruntime    3.499014e+04\nbudget     1.940498e+00\ndtype: float64\n"
     ]
    }
   ],
   "source": [
    "y = np.array(df.profit)\n",
    "x = np.column_stack((X.const, np.column_stack((X.runtime, X.budget))))\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def gradientloss(betas, y, x):\n",
    "    result = 0\n",
    "    for i in range(0, len(y)):\n",
    "        xb = np.dot(x[i], betas)\n",
    "        loss = (xb - y[i])**2\n",
    "        result += loss\n",
    "    return result\n",
    "\n",
    "bhat = np.zeros(len(x[0]))\n",
    "\n",
    "gradient_est = minimize(gradientloss, bhat, args=(y,x), method='nelder-mead')\n",
    "\n",
    "print('Gradient descent attempt:',gradient_est['x'] * np.std(y))\n",
    "print('True coefficients:',coeffs_)\n",
    "\n",
    "#It would be a stretch to say this is perfect but it seems to be approaching the correct coefficients."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPIPzivigAhB3FeR6Q96N8T",
   "collapsed_sections": [],
   "name": "Workshop: Maximum likelihood.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}